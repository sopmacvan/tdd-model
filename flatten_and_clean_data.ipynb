{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Flatten and clean <u>expert.json</u>, then merge abnormalities per xray"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "def preprocess_json(path):\n",
    "    \"\"\"\n",
    "    read json file path, preprocess, and return it as DataFrame\n",
    "\n",
    "    :param path: str\n",
    "    :return df: DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    def replace_none_to_null(text):\n",
    "        \"\"\"\n",
    "        replace none texts to null\n",
    "\n",
    "        :param text: str\n",
    "        :return text: str\n",
    "        \"\"\"\n",
    "        patterns = [r'\"None\"', r'\"none\"']\n",
    "        for p in patterns:\n",
    "            text = re.sub(p, 'null', text)\n",
    "\n",
    "        return text\n",
    "\n",
    "    with open(path) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # convert json data to str, replace none with null\n",
    "    data = json.dumps(data)\n",
    "    data = replace_none_to_null(data)\n",
    "\n",
    "    # convert str data back to json and create df\n",
    "    json_data = json.loads(data)\n",
    "    df = pd.json_normalize(json_data)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_abnormalities_per_xray(df_):\n",
    "    \"\"\"\n",
    "    group multiple rows of abnormality w/ the same id and merge them into one row\n",
    "\n",
    "    :param df_: DataFrame\n",
    "    :return merged_data: dict\n",
    "    \"\"\"\n",
    "\n",
    "    def merge_abnormalities_per_level():\n",
    "        \"\"\"\n",
    "        group multiple rows of abnormality w/c are separated by level characteristic and merge them into one row containing all\n",
    "\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        first_item = current_df.iloc[0]\n",
    "\n",
    "        external_id = first_item['.External ID']\n",
    "        description = first_item['.Description']\n",
    "        polygons = current_df['.polygons'].apply(str).unique()\n",
    "        polygons = [eval(p) for p in polygons]\n",
    "\n",
    "        level_one = current_df[current_df['value'] == 'level_four']['.value'].to_list()\n",
    "        level_two = current_df[current_df['value'] == 'level_one']['answer.value'].to_list()\n",
    "        level_three = current_df[current_df['value'] == 'level_two']['answer.value'].to_list()\n",
    "        level_four = current_df[current_df['value'] == 'level_three']['answer.value'].to_list()\n",
    "        level_five = current_df[current_df['value'] == 'level_four']['answer.value'].to_list()\n",
    "\n",
    "        merged_data['External ID'].append(external_id)\n",
    "        merged_data['Description'].append(description)\n",
    "        merged_data['polygons'].append(polygons)\n",
    "        merged_data['level_one'].append(level_one)\n",
    "        merged_data['level_two'].append(level_two)\n",
    "        merged_data['level_three'].append(level_three)\n",
    "        merged_data['level_four'].append(level_four)\n",
    "        merged_data['level_five'].append(level_five)\n",
    "\n",
    "    with_ab_ids = df_['.External ID'].unique()\n",
    "    merged_data = {'External ID': [],\n",
    "                   'Description': [],\n",
    "                   'polygons': [],\n",
    "                   'level_one': [],\n",
    "                   'level_two': [],\n",
    "                   'level_three': [],\n",
    "                   'level_four': [],\n",
    "                   'level_five': [],\n",
    "                   }\n",
    "    for id_ in with_ab_ids:\n",
    "        current_df = df_[df_['.External ID'] == id_]\n",
    "        merge_abnormalities_per_level()\n",
    "\n",
    "    return merged_data\n",
    "\n",
    "\n",
    "def give_empty_column(x, col):\n",
    "    \"\"\"\n",
    "    add an empty key 'col' into dictionary 'x'\n",
    "\n",
    "    :param x: dict\n",
    "    :param col: str\n",
    "    :return: dict\n",
    "    \"\"\"\n",
    "    if col not in x.keys():\n",
    "        x.update({col: None})\n",
    "    return x\n",
    "\n",
    "\n",
    "def remove_excess_and_keep_last(row):\n",
    "    \"\"\"\n",
    "    remove excess and keep last\n",
    "\n",
    "    :param row: Series\n",
    "    :return row: Series\n",
    "    \"\"\"\n",
    "\n",
    "    def no_duplicates():\n",
    "        \"\"\"check if there are no duplicates\"\"\"\n",
    "        original_len = len(row['level_five'])\n",
    "        removed_duplicates_len = len(set(row['level_five']))\n",
    "        return original_len == removed_duplicates_len\n",
    "\n",
    "    if no_duplicates():\n",
    "        return row\n",
    "\n",
    "    n = row['len_polygons']\n",
    "\n",
    "    while len(row['level_one']) > n:\n",
    "        row['level_one'].pop(0)\n",
    "    while len(row['level_two']) > n:\n",
    "        row['level_two'].pop(0)\n",
    "    while len(row['level_three']) > n:\n",
    "        row['level_three'].pop(0)\n",
    "    while len(row['level_four']) > n:\n",
    "        row['level_four'].pop(0)\n",
    "    while len(row['level_five']) > n:\n",
    "        row['level_five'].pop(0)\n",
    "\n",
    "    return row"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                   Description  \\\nExternal ID                                                      \n1009.JPG     Periapical radiolucency associated with tooth ...   \n1012.JPG     Pericoronal radiolucencies associated with too...   \n1015.JPG     Apical inflammatory changes associated with to...   \n1047.JPG     There are linear calcific flecks noted in the ...   \n1001.JPG     Multiple extractions sites in the maxilla and ...   \n\n                                                      polygons  \\\nExternal ID                                                      \n1009.JPG     [[[[595, 631], [594, 632], [591, 632], [590, 6...   \n1012.JPG     [[[[1226, 499], [1224, 501], [1224, 502], [122...   \n1015.JPG     [[[[493, 562], [492, 563], [491, 563], [490, 5...   \n1047.JPG     [[[[1473, 701], [1473, 706], [1472, 707], [147...   \n1001.JPG     [[[[267, 496], [265, 498], [264, 498], [261, 5...   \n\n                                             level_one  \\\nExternal ID                                              \n1009.JPG     [periapical, non-odontogenic, periapical]   \n1012.JPG                                 [pericoronal]   \n1015.JPG                                  [periapical]   \n1047.JPG                             [non-odontogenic]   \n1001.JPG          [periapical, periapical, periapical]   \n\n                                             level_two  \\\nExternal ID                                              \n1009.JPG     [well_defined, well_defined, ill_defined]   \n1012.JPG                                [well_defined]   \n1015.JPG                                 [ill_defined]   \n1047.JPG                                [well_defined]   \n1001.JPG                                [well_defined]   \n\n                                                   level_three  level_four  \\\nExternal ID                                                                  \n1009.JPG     [radiolucent, radiopaque, mixed-septae/calcifi...  [thinning]   \n1012.JPG                                         [radiolucent]  [thinning]   \n1015.JPG                          [mixed-septae/calcification]          []   \n1047.JPG                                          [radiopaque]          []   \n1001.JPG                                         [radiolucent]          []   \n\n                                                    level_five  len_polygons  \\\nExternal ID                                                                    \n1009.JPG     [benign_cyst_neoplasia, developmental, inflamm...             3   \n1012.JPG                               [benign_cyst_neoplasia]             1   \n1015.JPG                                        [inflammation]             1   \n1047.JPG                                  [metabolic/systemic]             1   \n1001.JPG         [benign_cyst_neoplasia, inflammation, trauma]             1   \n\n             len_abnormalities  \nExternal ID                     \n1009.JPG                     3  \n1012.JPG                     1  \n1015.JPG                     1  \n1047.JPG                     1  \n1001.JPG                     3  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Description</th>\n      <th>polygons</th>\n      <th>level_one</th>\n      <th>level_two</th>\n      <th>level_three</th>\n      <th>level_four</th>\n      <th>level_five</th>\n      <th>len_polygons</th>\n      <th>len_abnormalities</th>\n    </tr>\n    <tr>\n      <th>External ID</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1009.JPG</th>\n      <td>Periapical radiolucency associated with tooth ...</td>\n      <td>[[[[595, 631], [594, 632], [591, 632], [590, 6...</td>\n      <td>[periapical, non-odontogenic, periapical]</td>\n      <td>[well_defined, well_defined, ill_defined]</td>\n      <td>[radiolucent, radiopaque, mixed-septae/calcifi...</td>\n      <td>[thinning]</td>\n      <td>[benign_cyst_neoplasia, developmental, inflamm...</td>\n      <td>3</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1012.JPG</th>\n      <td>Pericoronal radiolucencies associated with too...</td>\n      <td>[[[[1226, 499], [1224, 501], [1224, 502], [122...</td>\n      <td>[pericoronal]</td>\n      <td>[well_defined]</td>\n      <td>[radiolucent]</td>\n      <td>[thinning]</td>\n      <td>[benign_cyst_neoplasia]</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1015.JPG</th>\n      <td>Apical inflammatory changes associated with to...</td>\n      <td>[[[[493, 562], [492, 563], [491, 563], [490, 5...</td>\n      <td>[periapical]</td>\n      <td>[ill_defined]</td>\n      <td>[mixed-septae/calcification]</td>\n      <td>[]</td>\n      <td>[inflammation]</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1047.JPG</th>\n      <td>There are linear calcific flecks noted in the ...</td>\n      <td>[[[[1473, 701], [1473, 706], [1472, 707], [147...</td>\n      <td>[non-odontogenic]</td>\n      <td>[well_defined]</td>\n      <td>[radiopaque]</td>\n      <td>[]</td>\n      <td>[metabolic/systemic]</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1001.JPG</th>\n      <td>Multiple extractions sites in the maxilla and ...</td>\n      <td>[[[[267, 496], [265, 498], [264, 498], [261, 5...</td>\n      <td>[periapical, periapical, periapical]</td>\n      <td>[well_defined]</td>\n      <td>[radiolucent]</td>\n      <td>[]</td>\n      <td>[benign_cyst_neoplasia, inflammation, trauma]</td>\n      <td>1</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = preprocess_json(\n",
    "    r\"Tufts Dental Database/Expert/expert.json\")\n",
    "\n",
    "# flatten Label.objects, fix separated rows\n",
    "df = pd.concat([df.drop(columns='Label.objects'), pd.json_normalize(df['Label.objects'])], axis=1)\n",
    "df_1 = df[df[1].notna()].drop(columns=[0, 2, 3]).rename(columns={1: 0})\n",
    "df_2 = df[df[2].notna()].drop(columns=[0, 1, 3]).rename(columns={2: 0})\n",
    "df_3 = df[df[3].notna()].drop(columns=[0, 1, 2]).rename(columns={3: 0})\n",
    "df = pd.concat([df, df_1, df_2, df_3], axis=0)\n",
    "df = df.drop(columns=[1, 2, 3, 'Label.classifications'])\n",
    "\n",
    "# add id and description to each dictionary of json objects\n",
    "df[0] = df.apply(lambda row: {**row[0], 'External ID': row['External ID'], 'Description': row['Description']}, axis=1)\n",
    "\n",
    "# create with_ab_df and fix the abnormality characteristic levels before merging back\n",
    "with_ab_df = pd.json_normalize(data=df[0], record_path='classifications',\n",
    "                               meta=['External ID', 'Description', 'title', 'value', 'polygons'],\n",
    "                               meta_prefix='.')\n",
    "cols = ['.External ID', '.Description', '.polygons', '.value', 'value', 'answer.value', 'answers']\n",
    "with_ab_df = with_ab_df[cols]\n",
    "levels_df_id = with_ab_df['.External ID'].unique()\n",
    "df = df[~df['External ID'].isin(levels_df_id)]\n",
    "df = df.drop(columns=[0])\n",
    "\n",
    "# flatten answers, fix separated answers (level three and four)\n",
    "with_ab_df['answers'] = with_ab_df['answers'].fillna(\"\").apply(list)\n",
    "with_ab_df = pd.concat([with_ab_df.drop(columns='answers'), pd.json_normalize(with_ab_df['answers'])], axis=1)\n",
    "not_nan_0 = with_ab_df[with_ab_df[0].notna()].index\n",
    "not_nan_1 = with_ab_df[with_ab_df[1].notna()].index\n",
    "not_nan_2 = with_ab_df[with_ab_df[2].notna()].index\n",
    "\n",
    "levels_df_0 = with_ab_df.loc[not_nan_0].drop(columns=[1, 2, 'answer.value']).rename(columns={0: 'answer.value'})\n",
    "levels_df_1 = with_ab_df.loc[not_nan_1].drop(columns=[0, 2, 'answer.value']).rename(columns={1: 'answer.value'})\n",
    "levels_df_2 = with_ab_df.loc[not_nan_2].drop(columns=[0, 1, 'answer.value']).rename(columns={2: 'answer.value'})\n",
    "\n",
    "levels_df_0['answer.value'] = levels_df_0['answer.value'].apply(lambda x: pd.json_normalize(x)['value'])\n",
    "levels_df_1['answer.value'] = levels_df_1['answer.value'].apply(lambda x: pd.json_normalize(x)['value'])\n",
    "levels_df_2['answer.value'] = levels_df_2['answer.value'].apply(lambda x: pd.json_normalize(x)['value'])\n",
    "\n",
    "with_ab_df = with_ab_df.drop(columns=[0, 1, 2])\n",
    "with_ab_df = pd.concat([with_ab_df, levels_df_0, levels_df_1, levels_df_2], axis=0)\n",
    "with_ab_df = with_ab_df.dropna(subset=['answer.value'])\n",
    "\n",
    "# merge abnormalities per xray id\n",
    "merged_ab_df = pd.DataFrame(merge_abnormalities_per_xray(with_ab_df))\n",
    "# merge back all\n",
    "expert_merged_df = pd.concat([df, merged_ab_df], axis=0)\n",
    "\n",
    "# cleanup\n",
    "# remove duplicates\n",
    "expert_merged_df.drop_duplicates(subset='External ID', inplace=True)\n",
    "expert_merged_df.set_index('External ID', inplace=True)\n",
    "# remove excess per level characteristic\n",
    "expert_merged_df['len_polygons'] = expert_merged_df['polygons'].apply(lambda x: len(x) if x is not np.nan else 0)\n",
    "expert_merged_df['len_abnormalities'] = expert_merged_df['level_five'].apply(lambda x: len(x) if x is not np.nan else 0)\n",
    "with_excess = expert_merged_df[expert_merged_df['len_abnormalities'] > expert_merged_df['len_polygons']].index\n",
    "expert_merged_df.loc[with_excess] = expert_merged_df.loc[with_excess].apply(lambda x: remove_excess_and_keep_last(x),\n",
    "                                                                            axis=1)\n",
    "# # drop unsure classes\n",
    "expert_merged_df['len_polygons'] = expert_merged_df['polygons'].apply(lambda x: len(x) if x is not np.nan else 0)\n",
    "expert_merged_df['len_abnormalities'] = expert_merged_df['level_five'].apply(lambda x: len(x) if x is not np.nan else 0)\n",
    "# with_unsure = expert_merged_df[expert_merged_df['len_abnormalities'] > expert_merged_df['len_polygons']].index\n",
    "# expert_merged_df.drop(index=with_unsure, inplace=True)\n",
    "# # drop unclassified abnormalities\n",
    "# with_unclassified = expert_merged_df[expert_merged_df['len_abnormalities'] < expert_merged_df['len_polygons']].index\n",
    "# expert_merged_df.drop(index=with_unclassified, inplace=True)\n",
    "\n",
    "\n",
    "# save to csv\n",
    "expert_merged_df.to_csv('Tufts Dental Database/Expert/expert_merged.csv')\n",
    "\n",
    "expert_merged_df.tail()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Flatten and clean <u>student.json</u>, then merge abnormalities per xray"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                   Description  \\\nExternal ID                                                      \n1010.JPG     There is a large periapical radiolucency on to...   \n709.JPG      There is a periapical radiolucency on tooth nu...   \n741.JPG      There is a non-odontogenic well-defined radiop...   \n840.JPG      On the left side by the bifurcation of the car...   \n205.JPG      There is a periapical well-defined radiolucent...   \n\n                                                      polygons  \\\nExternal ID                                                      \n1010.JPG     [[[[863, 587], [863, 590], [861, 592], [861, 5...   \n709.JPG      [[[[510, 580], [509, 581], [508, 581], [507, 5...   \n741.JPG      [[[[140, 766], [141, 767], [141, 768], [142, 7...   \n840.JPG      [[[[1516, 724], [1517, 725], [1515, 727], [151...   \n205.JPG      [[[[428, 577], [429, 576], [431, 578], [431, 5...   \n\n                     level_one       level_two    level_three  \\\nExternal ID                                                     \n1010.JPG          [periapical]  [well_defined]  [radiolucent]   \n709.JPG           [periapical]  [well_defined]  [radiolucent]   \n741.JPG      [non-odontogenic]  [well_defined]   [radiopaque]   \n840.JPG      [non-odontogenic]  [well_defined]   [radiopaque]   \n205.JPG           [periapical]  [well_defined]  [radiolucent]   \n\n                      level_four               level_five  len_polygons  \\\nExternal ID                                                               \n1010.JPG     [osseous_expansion]  [benign_cyst_neoplasia]             1   \n709.JPG                       []           [inflammation]             1   \n741.JPG                       []     [metabolic/systemic]             1   \n840.JPG                       []     [metabolic/systemic]             1   \n205.JPG                       []           [inflammation]             1   \n\n             len_abnormalities  \nExternal ID                     \n1010.JPG                     1  \n709.JPG                      1  \n741.JPG                      1  \n840.JPG                      1  \n205.JPG                      1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Description</th>\n      <th>polygons</th>\n      <th>level_one</th>\n      <th>level_two</th>\n      <th>level_three</th>\n      <th>level_four</th>\n      <th>level_five</th>\n      <th>len_polygons</th>\n      <th>len_abnormalities</th>\n    </tr>\n    <tr>\n      <th>External ID</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1010.JPG</th>\n      <td>There is a large periapical radiolucency on to...</td>\n      <td>[[[[863, 587], [863, 590], [861, 592], [861, 5...</td>\n      <td>[periapical]</td>\n      <td>[well_defined]</td>\n      <td>[radiolucent]</td>\n      <td>[osseous_expansion]</td>\n      <td>[benign_cyst_neoplasia]</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>709.JPG</th>\n      <td>There is a periapical radiolucency on tooth nu...</td>\n      <td>[[[[510, 580], [509, 581], [508, 581], [507, 5...</td>\n      <td>[periapical]</td>\n      <td>[well_defined]</td>\n      <td>[radiolucent]</td>\n      <td>[]</td>\n      <td>[inflammation]</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>741.JPG</th>\n      <td>There is a non-odontogenic well-defined radiop...</td>\n      <td>[[[[140, 766], [141, 767], [141, 768], [142, 7...</td>\n      <td>[non-odontogenic]</td>\n      <td>[well_defined]</td>\n      <td>[radiopaque]</td>\n      <td>[]</td>\n      <td>[metabolic/systemic]</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>840.JPG</th>\n      <td>On the left side by the bifurcation of the car...</td>\n      <td>[[[[1516, 724], [1517, 725], [1515, 727], [151...</td>\n      <td>[non-odontogenic]</td>\n      <td>[well_defined]</td>\n      <td>[radiopaque]</td>\n      <td>[]</td>\n      <td>[metabolic/systemic]</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>205.JPG</th>\n      <td>There is a periapical well-defined radiolucent...</td>\n      <td>[[[[428, 577], [429, 576], [431, 578], [431, 5...</td>\n      <td>[periapical]</td>\n      <td>[well_defined]</td>\n      <td>[radiolucent]</td>\n      <td>[]</td>\n      <td>[inflammation]</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = preprocess_json(r\"Tufts Dental Database/Student/student.json\")\n",
    "\n",
    "# flatten Label.objects, fix separated rows\n",
    "df = pd.concat([df.drop(columns='Label.objects'), pd.json_normalize(df['Label.objects'])], axis=1)\n",
    "df_1 = df[df[1].notna()].drop(columns=[0]).rename(columns={1: 0})\n",
    "# df_2 = df[df[2].notna()].drop(columns=[0, 1, 3]).rename(columns={2: 0})\n",
    "# df_3 = df[df[3].notna()].drop(columns=[0, 1, 2]).rename(columns={3: 0})\n",
    "# df = pd.concat([df, df_1, df_2, df_3], axis=0)\n",
    "df = pd.concat([df, df_1], axis=0)\n",
    "df = df.drop(columns=[1, 'Label.classifications'])\n",
    "\n",
    "# add id and description to each dictionary of json objects\n",
    "df[0] = df.apply(lambda row: {**row[0], 'External ID': row['External ID'], 'Description': row['Description']}, axis=1)\n",
    "\n",
    "df[0] = df[0].apply(lambda x: give_empty_column(x, 'classifications'))\n",
    "\n",
    "# create with_ab_df and fix the abnormality characteristic levels before merging back\n",
    "with_ab_df = pd.json_normalize(data=df[0], record_path='classifications',\n",
    "                               meta=['External ID', 'Description', 'title', 'value', 'polygons'],\n",
    "                               meta_prefix='.', errors='ignore')\n",
    "cols = ['.External ID', '.Description', '.polygons', '.value', 'value', 'answer.value', 'answers']\n",
    "with_ab_df = with_ab_df[cols]\n",
    "levels_df_id = with_ab_df['.External ID'].unique()\n",
    "df = df[~df['External ID'].isin(levels_df_id)]\n",
    "df = df.drop(columns=[0])\n",
    "\n",
    "# flatten answers, fix separated answers (level three and four)\n",
    "with_ab_df['answers'] = with_ab_df['answers'].fillna(\"\").apply(list)\n",
    "with_ab_df = pd.concat([with_ab_df.drop(columns='answers'), pd.json_normalize(with_ab_df['answers'])], axis=1)\n",
    "not_nan_0 = with_ab_df[with_ab_df[0].notna()].index\n",
    "not_nan_1 = with_ab_df[with_ab_df[1].notna()].index\n",
    "not_nan_2 = with_ab_df[with_ab_df[2].notna()].index\n",
    "\n",
    "levels_df_0 = with_ab_df.loc[not_nan_0].drop(columns=[1, 2, 'answer.value']).rename(columns={0: 'answer.value'})\n",
    "levels_df_1 = with_ab_df.loc[not_nan_1].drop(columns=[0, 2, 'answer.value']).rename(columns={1: 'answer.value'})\n",
    "levels_df_2 = with_ab_df.loc[not_nan_2].drop(columns=[0, 1, 'answer.value']).rename(columns={2: 'answer.value'})\n",
    "\n",
    "levels_df_0['answer.value'] = levels_df_0['answer.value'].apply(lambda x: pd.json_normalize(x)['value'])\n",
    "levels_df_1['answer.value'] = levels_df_1['answer.value'].apply(lambda x: pd.json_normalize(x)['value'])\n",
    "levels_df_2['answer.value'] = levels_df_2['answer.value'].apply(lambda x: pd.json_normalize(x)['value'])\n",
    "\n",
    "with_ab_df = with_ab_df.drop(columns=[0, 1, 2])\n",
    "with_ab_df = pd.concat([with_ab_df, levels_df_0, levels_df_1, levels_df_2], axis=0)\n",
    "with_ab_df = with_ab_df.dropna(subset=['answer.value'])\n",
    "\n",
    "# merge abnormalities per xray id\n",
    "merged_ab_df = pd.DataFrame(merge_abnormalities_per_xray(with_ab_df))\n",
    "# merge back all\n",
    "student_merged_df = pd.concat([df, merged_ab_df], axis=0)\n",
    "\n",
    "# cleanup\n",
    "# remove duplicates\n",
    "student_merged_df.drop_duplicates(subset='External ID', inplace=True)\n",
    "student_merged_df.set_index('External ID', inplace=True)\n",
    "# remove excess per level characteristic\n",
    "student_merged_df['len_polygons'] = student_merged_df['polygons'].apply(lambda x: len(x) if x is not np.nan else 0)\n",
    "student_merged_df['len_abnormalities'] = student_merged_df['level_five'].apply(\n",
    "    lambda x: len(x) if x is not np.nan else 0)\n",
    "with_excess = student_merged_df[student_merged_df['len_abnormalities'] > student_merged_df['len_polygons']].index\n",
    "student_merged_df.loc[with_excess] = student_merged_df.loc[with_excess].apply(lambda x: remove_excess_and_keep_last(x),\n",
    "                                                                              axis=1)\n",
    "# # drop unsure classes\n",
    "student_merged_df['len_polygons'] = student_merged_df['polygons'].apply(lambda x: len(x) if x is not np.nan else 0)\n",
    "student_merged_df['len_abnormalities'] = student_merged_df['level_five'].apply(\n",
    "    lambda x: len(x) if x is not np.nan else 0)\n",
    "# with_unsure = student_merged_df[student_merged_df['len_abnormalities'] > student_merged_df['len_polygons']].index\n",
    "# student_merged_df.drop(index=with_unsure, inplace=True)\n",
    "# # drop unclassified abnormalities\n",
    "# with_unclassified = student_merged_df[student_merged_df['len_abnormalities'] < student_merged_df['len_polygons']].index\n",
    "# student_merged_df.drop(index=with_unclassified, inplace=True)\n",
    "\n",
    "# save to csv\n",
    "student_merged_df.to_csv('Tufts Dental Database/Student/student_merged.csv')\n",
    "student_merged_df.tail()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ~~Separate abnormalities per xray~~  (DO NOT USE, improperly implemented)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "def separate_abnormalities_per_xray(df_, ):\n",
    "    \"\"\"\n",
    "    separate abnormalities and their characteristics per xray\n",
    "\n",
    "    :param df_: DataFrame\n",
    "    :return separated_data: dict\n",
    "    \"\"\"\n",
    "\n",
    "    def add_separated_data():\n",
    "        \"\"\"\n",
    "        for each abnormality in an xray, separate and add this data to separated_data dict\n",
    "\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        external_id = current_df.name\n",
    "        description = current_df['Description']\n",
    "\n",
    "        n = current_df['len_abnormalities']\n",
    "\n",
    "        if n == 0:\n",
    "            separated_data['External ID'].append(external_id)\n",
    "            separated_data['Description'].append(description)\n",
    "            separated_data['polygons'].append(np.nan)\n",
    "            separated_data['level_one'].append(np.nan)\n",
    "            separated_data['level_two'].append(np.nan)\n",
    "            separated_data['level_three'].append(np.nan)\n",
    "            separated_data['level_four'].append(np.nan)\n",
    "            separated_data['level_five'].append(np.nan)\n",
    "            return\n",
    "\n",
    "        polygons = current_df['polygons']\n",
    "        level_one = current_df['level_one']\n",
    "        level_two = current_df['level_two']\n",
    "        level_three = current_df['level_three']\n",
    "        level_four = current_df['level_four']\n",
    "        level_five = current_df['level_five']\n",
    "\n",
    "        poly_n = len(polygons)\n",
    "        l1_n = len(level_one)\n",
    "        l2_n = len(level_two)\n",
    "        l3_n = len(level_three)\n",
    "        l4_n = len(level_four)\n",
    "        l5_n = len(level_five)\n",
    "\n",
    "        for i in range(n):\n",
    "            separated_data['External ID'].append(external_id)\n",
    "            separated_data['Description'].append(description)\n",
    "            poly_i = i if i < poly_n else -1\n",
    "            l1_i = i if i < l1_n else -1\n",
    "            l2_i = i if i < l2_n else -1\n",
    "            l3_i = i if i < l3_n else -1\n",
    "            l4_i = i if i < l4_n else -1\n",
    "            l5_i = i if i < l5_n else -1\n",
    "            separated_data['polygons'].append(polygons[poly_i] if poly_n > 0 else np.nan)\n",
    "            separated_data['level_one'].append(level_one[l1_i] if l1_n > 0 else np.nan)\n",
    "            separated_data['level_two'].append(level_two[l2_i] if l2_n > 0 else np.nan)\n",
    "            separated_data['level_three'].append(level_three[l3_i] if l3_n > 0 else np.nan)\n",
    "            separated_data['level_four'].append(level_four[l4_i] if l4_n > 0 else np.nan)\n",
    "            separated_data['level_five'].append(level_five[l5_i] if l5_n > 0 else np.nan)\n",
    "\n",
    "    separated_data = {'External ID': [],\n",
    "                      'Description': [],\n",
    "                      'polygons': [],\n",
    "                      'level_one': [],\n",
    "                      'level_two': [],\n",
    "                      'level_three': [],\n",
    "                      'level_four': [],\n",
    "                      'level_five': [],\n",
    "                      }\n",
    "    df_ids = df_.index.unique()\n",
    "    for id_ in df_ids:\n",
    "        current_df = df_.loc[id_]\n",
    "        add_separated_data()\n",
    "\n",
    "    return separated_data\n",
    "\n",
    "\n",
    "# separate abnormalities per xray\n",
    "expert_separated_df = pd.DataFrame(separate_abnormalities_per_xray(expert_merged_df))\n",
    "student_separated_df = pd.DataFrame(separate_abnormalities_per_xray(student_merged_df))\n",
    "\n",
    "# set index to external id\n",
    "expert_separated_df.set_index('External ID', inplace=True)\n",
    "student_separated_df.set_index('External ID', inplace=True)\n",
    "\n",
    "# save to csv\n",
    "expert_separated_df.to_csv('Tufts Dental Database/Expert/expert_separated.csv')\n",
    "student_separated_df.to_csv('Tufts Dental Database/Student/student_separated.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Check count"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "data": {
      "text/plain": "count               normal abnormal total\nannotator type                           \nexpert    merged       660      340  1000\n          separated    NaN      NaN   NaN\nstudent   merged       799      201  1000\n          separated    NaN      NaN   NaN",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>count</th>\n      <th>normal</th>\n      <th>abnormal</th>\n      <th>total</th>\n    </tr>\n    <tr>\n      <th>annotator</th>\n      <th>type</th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"2\" valign=\"top\">expert</th>\n      <th>merged</th>\n      <td>660</td>\n      <td>340</td>\n      <td>1000</td>\n    </tr>\n    <tr>\n      <th>separated</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th rowspan=\"2\" valign=\"top\">student</th>\n      <th>merged</th>\n      <td>799</td>\n      <td>201</td>\n      <td>1000</td>\n    </tr>\n    <tr>\n      <th>separated</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count expert merged\n",
    "expert_merged_total = len(expert_merged_df)\n",
    "expert_merged_abnormal = len(expert_merged_df[expert_merged_df['level_five'].notna()])\n",
    "expert_merged_normal = expert_merged_total - expert_merged_abnormal\n",
    "# # count expert separated\n",
    "# expert_separated_total = len(expert_separated_df)\n",
    "# expert_separated_abnormal = len(expert_separated_df[expert_separated_df['level_five'].notna()])\n",
    "# expert_separated_normal = expert_separated_total - expert_separated_abnormal\n",
    "# count student merged\n",
    "student_merged_total = len(student_merged_df)\n",
    "student_merged_abnormal = len(student_merged_df[student_merged_df['level_five'].notna()])\n",
    "student_merged_normal = student_merged_total - student_merged_abnormal\n",
    "# # count student separated\n",
    "# student_separated_total = len(student_separated_df)\n",
    "# student_separated_abnormal = len(student_separated_df[student_separated_df['level_five'].notna()])\n",
    "# student_separated_normal = student_separated_total - student_separated_abnormal\n",
    "\n",
    "\n",
    "# Create the hierarchical index\n",
    "index = pd.MultiIndex.from_product([['expert', 'student'], ['merged', 'separated']], names=['annotator', 'type'])\n",
    "columns = pd.Index(['normal', 'abnormal', 'total'], name='count')\n",
    "# Create the DataFrame with the index and columns\n",
    "df = pd.DataFrame(index=index, columns=columns)\n",
    "# Add the data to the DataFrame\n",
    "df.loc[('expert', 'merged'), 'normal'] = expert_merged_normal\n",
    "df.loc[('expert', 'merged'), 'abnormal'] = expert_merged_abnormal\n",
    "df.loc[('expert', 'merged'), 'total'] = expert_merged_total\n",
    "# df.loc[('expert', 'separated'), 'normal'] = expert_separated_normal\n",
    "# df.loc[('expert', 'separated'), 'abnormal'] = expert_separated_abnormal\n",
    "# df.loc[('expert', 'separated'), 'total'] = expert_separated_total\n",
    "df.loc[('student', 'merged'), 'normal'] = student_merged_normal\n",
    "df.loc[('student', 'merged'), 'abnormal'] = student_merged_abnormal\n",
    "df.loc[('student', 'merged'), 'total'] = student_merged_total\n",
    "# df.loc[('student', 'separated'), 'normal'] = student_separated_normal\n",
    "# df.loc[('student', 'separated'), 'abnormal'] = student_separated_abnormal\n",
    "# df.loc[('student', 'separated'), 'total'] = student_separated_total\n",
    "\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Additional Notes\n",
    "\n",
    "The annotations to the following images have multiple abnormalities in the same exact area (same mask).\n",
    "\n",
    "- expert_df\n",
    "    - ['935.JPG', '196.JPG', '906.JPG', '938.JPG', '521.JPG', '559.JPG',\n",
    "       '378.JPG', '992.JPG', '995.JPG', '255.JPG', '288.JPG', '612.JPG',\n",
    "       '551.JPG', '778.JPG', '810.JPG', '373.JPG', '566.JPG', '630.JPG',\n",
    "       '315.JPG', '105.JPG', '336.JPG', '721.JPG', '371.JPG', '532.JPG',\n",
    "       '1036.JPG', '1037.JPG', '1001.JPG']\n",
    "\n",
    "- student_df\n",
    "    - ['245.JPG', '761.JPG', '315.JPG', '925.JPG', '371.JPG', '357.JPG',\n",
    "       '780.JPG', '224.JPG', '548.JPG', '355.JPG', '44.JPG', '108.JPG',\n",
    "       '627.JPG', '951.JPG', '1011.JPG']"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
